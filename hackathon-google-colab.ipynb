{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "443dQsLqu4U-"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Conv2d, MaxPool2d, Dropout, Linear, ReLU, CrossEntropyLoss\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torchvision\n",
        "import pathlib\n",
        "import pickle\n",
        "import torch\n",
        "import os\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHiL9n2lvTi2",
        "outputId": "0a306588-1cb6-46e8-9ac5-bb9a153392f7"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoqqhvnMvbR0",
        "outputId": "d8045e0e-87d9-418c-de3d-52341a56d4d2"
      },
      "outputs": [],
      "source": [
        "!unzip drive/MyDrive/hackathon_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHj6C5sTu98l"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "images_path = 'bev_classification/images'\n",
        "test_path = 'bev_classification/images/test'\n",
        "train_path = 'bev_classification/images/train'\n",
        "\n",
        "if not pathlib.Path(test_path).exists():\n",
        "    pathlib.Path(test_path).mkdir()\n",
        "\n",
        "if not pathlib.Path(train_path).exists():\n",
        "    pathlib.Path(train_path).mkdir()\n",
        "\n",
        "for file in pathlib.Path(images_path).iterdir():\n",
        "    if file.name == '.DS_Store' or \\\n",
        "        file == test_path or \\\n",
        "        file == train_path:\n",
        "        continue\n",
        "\n",
        "    if 'test' in file.name:\n",
        "        for data in pathlib.Path(file).iterdir():\n",
        "            # Only iterate over the class directories\n",
        "            if 'image-datasets' == data.name or 'input' in data.name or data.name == '.DS_Store': continue\n",
        "\n",
        "            if pathlib.Path(f'{test_path}/{data.name}').exists():\n",
        "                # print(data.name)\n",
        "                for f in pathlib.Path(data).iterdir():\n",
        "                    pathlib.Path(f).rename(f'{test_path}/{data.name}/{f.name}')\n",
        "\n",
        "            else:\n",
        "                pathlib.Path(data).rename(f'{test_path}/{data.name}')\n",
        "            # print(f'{test_path}/{data.name}')\n",
        "    else:\n",
        "        for data in pathlib.Path(file).iterdir():\n",
        "            # Only iterate over the class directories\n",
        "            if 'image-datasets' == data.name or 'input' in data.name or data.name == '.DS_Store': continue\n",
        "\n",
        "            if pathlib.Path(f'{train_path}/{data.name}').exists():\n",
        "\n",
        "                for f in pathlib.Path(data).iterdir():\n",
        "                    pathlib.Path(f).rename(f'{train_path}/{data.name}/{f.name}')\n",
        "\n",
        "            else:\n",
        "                pathlib.Path(data).rename(f'{train_path}/{data.name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmXfAiFzvB-4"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "train_path = 'bev_classification/images/train'\n",
        "val_path = 'bev_classification/images/val'\n",
        "\n",
        "if not pathlib.Path(val_path).exists():\n",
        "    pathlib.Path(val_path).mkdir()\n",
        "\n",
        "for id in pathlib.Path(train_path).iterdir():\n",
        "    if not pathlib.Path(f\"{val_path}/{id.name}\").exists():\n",
        "        pathlib.Path(f\"{val_path}/{id.name}\").mkdir()\n",
        "\n",
        "    files = [file for file in pathlib.Path(id).iterdir()]\n",
        "    val_files = files[:len(files)//8]\n",
        "\n",
        "    [pathlib.Path(f).rename(f'{val_path}/{id.name}/{f.name}') for f in val_files]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-b-rtz3u4U_"
      },
      "outputs": [],
      "source": [
        "class BevDataset(Dataset):\n",
        "  def __init__(self, root, size=224, split='train'):\n",
        "    self.split = split\n",
        "    postfix = split\n",
        "    root = os.path.join(root, 'bev_classification', 'images')\n",
        "    self.dataset_folder = torchvision.datasets.ImageFolder(os.path.join(root, postfix) ,transform = transforms.Compose([transforms.Resize((size,size)),transforms.ToTensor()]))\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img = self.dataset_folder[index]\n",
        "    path = self.dataset_folder.imgs[index]\n",
        "    return img[0], img[1], path[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXtgcEFYu4VA"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class VGGIntermediate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGGIntermediate, self).__init__()\n",
        "        self.vgg = models.vgg19(pretrained=True)\n",
        "        self.set_up_vgg()\n",
        "\n",
        "    def set_up_vgg(self):\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        num_features = self.vgg.classifier[-1].in_features  # Get the number of input features for the final layer\n",
        "        self.vgg.classifier[-1] = Linear(num_features, 99)\n",
        "        # Optionally, you may want to initialize the new layer weights\n",
        "        # Initialize weights with Xavier initialization\n",
        "        torch.nn.init.xavier_uniform_(self.vgg.classifier[-1].weight)\n",
        "        # Optionally, initialize biases to zeros\n",
        "        torch.nn.init.zeros_(self.vgg.classifier[-1].bias)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.vgg(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b29JTqbu4VA",
        "outputId": "d73498d4-ad4d-4c63-a03f-7da91592f525"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\repoch: 1 batch: 2025 accuracy: 22.64% val accuracy 39.20% loss: 3.2424 val loss: 2.3560:   4%|▍         | 2026/48580 [05:31<1:10:22, 11.03it/s]"
          ]
        }
      ],
      "source": [
        "# Hyper parameters\n",
        "# With batch_size 50, there will be 1776 iterations over the dataset per epoch\n",
        "batch_size = 20\n",
        "num_epochs = 5\n",
        "lr = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mai7ooHpu4VA"
      },
      "outputs": [],
      "source": [
        "train_losses = [0]\n",
        "train_accuracy = [0]\n",
        "\n",
        "val_losses = [0]\n",
        "val_accuracy = [0]\n",
        "\n",
        "model_path = 'model/cuda-model.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYBTJO4Eu4VA",
        "outputId": "c1392cc5-bc5a-49c7-8190-34dd73507ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Found!\n",
            "Loading model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch: 5 batch: 3886 accuracy: 64.60% val accuracy 71.68% loss: 1.2531 val loss: 1.0059: 100%|██████████| 19435/19435 [2:12:10<00:00,  2.45it/s]\n",
            "epoch: 1 batch: 775 accuracy: 64.60% val accuracy 71.68% loss: 1.2531 val loss: 1.0059:   4%|▍         | 776/19435 [02:47<1:04:23,  4.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val batch: 553 val accuracy: 100.00% val loss: 0.2774: 100%|██████████| 554/554 [15:34<00:00,  1.69s/it]\n",
            "epoch: 1 batch: 1552 accuracy: 65.42% val accuracy 72.22% loss: 1.2173 val loss: 0.9801:   8%|▊         | 1553/19435 [07:36<1:01:56,  4.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val batch: 553 val accuracy: 100.00% val loss: 0.9236: 100%|██████████| 554/554 [04:49<00:00,  1.91it/s]\n",
            "epoch: 1 batch: 2329 accuracy: 65.51% val accuracy 71.97% loss: 1.2149 val loss: 0.9837:  12%|█▏        | 2330/19435 [12:24<58:53,  4.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val batch: 553 val accuracy: 100.00% val loss: 0.6252: 100%|██████████| 554/554 [04:47<00:00,  1.93it/s]\n",
            "epoch: 1 batch: 3106 accuracy: 65.50% val accuracy 72.14% loss: 1.2184 val loss: 0.9725:  16%|█▌        | 3107/19435 [17:11<1:03:39,  4.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val batch: 553 val accuracy: 100.00% val loss: 0.0323: 100%|██████████| 554/554 [04:47<00:00,  1.92it/s]\n",
            "epoch: 1 batch: 3883 accuracy: 65.54% val accuracy 72.27% loss: 1.2204 val loss: 0.9670:  20%|█▉        | 3884/19435 [21:59<53:36,  4.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val batch: 553 val accuracy: 100.00% val loss: 0.1130: 100%|██████████| 554/554 [04:47<00:00,  1.92it/s]\n",
            "epoch: 1 batch: 3886 accuracy: 65.37% val accuracy 73.25% loss: 1.2228 val loss: 0.9494:  20%|██        | 3887/19435 [23:59<109:23:14, 25.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model...\n",
            "Model saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch: 2 batch: 775 accuracy: 65.37% val accuracy 73.25% loss: 1.2228 val loss: 0.9494:  24%|██▍       | 4663/19435 [26:54<54:48,  4.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val batch: 553 val accuracy: 100.00% val loss: 0.2532: 100%|██████████| 554/554 [04:55<00:00,  1.88it/s]\n",
            "epoch: 2 batch: 1552 accuracy: 66.29% val accuracy 73.19% loss: 1.1852 val loss: 0.9507:  28%|██▊       | 5440/19435 [31:44<49:10,  4.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val batch: 553 val accuracy: 100.00% val loss: 0.5377: 100%|██████████| 554/554 [04:49<00:00,  1.91it/s]\n",
            "epoch: 2 batch: 2329 accuracy: 66.13% val accuracy 72.99% loss: 1.1884 val loss: 0.9416:  32%|███▏      | 6217/19435 [36:32<49:51,  4.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val batch: 553 val accuracy: 100.00% val loss: 0.1898: 100%|██████████| 554/554 [04:48<00:00,  1.92it/s]\n",
            "epoch: 2 batch: 3106 accuracy: 65.74% val accuracy 72.93% loss: 1.2002 val loss: 0.9352:  36%|███▌      | 6994/19435 [41:18<43:41,  4.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val batch: 553 val accuracy: 100.00% val loss: 0.0033: 100%|██████████| 554/554 [04:46<00:00,  1.93it/s]\n",
            "epoch: 2 batch: 3856 accuracy: 65.64% val accuracy 73.49% loss: 1.2013 val loss: 0.9265:  40%|███▉      | 7744/19435 [45:58<44:29,  4.38it/s]"
          ]
        }
      ],
      "source": [
        "def fine_tune():\n",
        "    try:\n",
        "        gc.collect()\n",
        "        device = torch.device('cuda')\n",
        "        if not pathlib.Path(model_path).exists():\n",
        "            model = VGGIntermediate().to(device)\n",
        "        else:\n",
        "            print('Model Found!')\n",
        "            print('Loading model...')\n",
        "            with open(model_path, 'rb') as f:\n",
        "                model = pickle.load(f).to(device)\n",
        "\n",
        "        print()\n",
        "        train_loader = DataLoader(BevDataset('.'), batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(BevDataset('.', split='val'), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Only have 10 validation checks per epoch\n",
        "        val_check = len(train_loader) // 5\n",
        "\n",
        "        criterion = CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        loop = tqdm(total=len(train_loader)*num_epochs, position=0)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_step_losses = []\n",
        "            train_step_accuracy = []\n",
        "            for batch, (x, y_truth, _) in enumerate(train_loader):\n",
        "                x, y_truth = x.to(device), y_truth.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_hat = model(x)\n",
        "\n",
        "                accuracy = (y_hat.argmax(1) == y_truth).float().mean()\n",
        "                train_step_accuracy.append(accuracy.item())\n",
        "\n",
        "                loss = criterion(y_hat, y_truth)\n",
        "                train_step_losses.append(loss.item())\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if (batch + 1) % val_check == 0:\n",
        "                    print('Validation check')\n",
        "                    val_loop = tqdm(total=len(val_loader), position=0)\n",
        "\n",
        "                    val_batch_loss = []\n",
        "                    val_batch_accuracy = []\n",
        "                    for batch, (x, y_truth, _) in enumerate(val_loader):\n",
        "                        x, y_truth = x.to(device), y_truth.to(device)\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                        y_hat = model(x)\n",
        "\n",
        "                        accuracy = (F.softmax(y_hat,1).argmax(1) == y_truth).float().mean()\n",
        "                        val_batch_accuracy.append(accuracy.item())\n",
        "\n",
        "                        loss = criterion(y_hat, y_truth)\n",
        "                        val_batch_loss.append(loss.item())\n",
        "\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        val_loop.update(1)\n",
        "                        val_loop.set_description(f'val batch: {batch} val accuracy: {accuracy*100:.2f}% val loss: {loss:.4f}')\n",
        "\n",
        "                    val_losses.append(sum(val_batch_loss) / len(val_batch_loss))\n",
        "                    val_accuracy.append(sum(val_batch_accuracy) / len(val_batch_accuracy))\n",
        "\n",
        "                    train_losses.append(sum(train_step_losses) / len(train_step_losses))\n",
        "                    train_accuracy.append(sum(train_step_accuracy) / len(train_step_accuracy))\n",
        "\n",
        "                loop.update(1)\n",
        "                loop.set_description(f'epoch: {epoch+1} batch: {batch} accuracy: {train_accuracy[-1]*100:.2f}% val accuracy {val_accuracy[-1]*100:.2f}% loss: {train_losses[-1]:.4f} val loss: {val_losses[-1]:.4f}')\n",
        "\n",
        "            print('Saving model...')\n",
        "            with open(model_path, 'wb') as f:\n",
        "                pickle.dump(model, f)\n",
        "            print('Model saved.')\n",
        "    except KeyboardInterrupt:\n",
        "        print('Saving model...')\n",
        "        with open(model_path, 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        print('Model saved.')\n",
        "\n",
        "# fine_tune()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJiWb8aLWRnX"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "\n",
        "device = torch.device('mps')\n",
        "print('Model Found!')\n",
        "print('Loading model...')\n",
        "with open(model_path, 'rb') as f:\n",
        "    model = pickle.load(f).to(device)\n",
        "test_dataset = BevDataset('.', split='test')\n",
        "test_loader = DataLoader(BevDataset('.', split='test'), batch_size=batch_size, shuffle=True)\n",
        "test_classes = test_dataset.dataset_folder.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNnNepDFWRXf"
      },
      "outputs": [],
      "source": [
        "def write_predictions(y_hat, paths, f):\n",
        "    class_nums = F.softmax(y_hat,1).argmax(1)\n",
        "    class_labels = [test_classes[c] for c in class_nums]\n",
        "    for path, label in zip(paths, class_labels):\n",
        "        f.write(f'{path},{label}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nWCi7FiWC8F"
      },
      "outputs": [],
      "source": [
        "def write_five_predictions(y_hat, paths, f):\n",
        "    for i in range(y_hat.size()[0]):\n",
        "        class_nums = torch.topk(y_hat[i,:].flatten(), 5).indices\n",
        "        class_labels = [str(test_classes[c]) for c in class_nums]\n",
        "        f.write(f'{paths[i]},{\",\".join(class_labels)}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s58ljMnsWGjT"
      },
      "outputs": [],
      "source": [
        "with open('./drive/MyDrive/test_edited.txt', 'w') as f, open('./drive/MyDrive/test_edited_five.txt', 'w') as f_f:\n",
        "    loop = tqdm(total=len(test_loader), position=0)\n",
        "    for x, _, path in test_loader:\n",
        "        y_hat = model(x.to(device))\n",
        "        write_predictions(y_hat, path, f)\n",
        "        write_five_predictions(y_hat, path, f_f)\n",
        "        loop.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiK6bfL0WZQs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
